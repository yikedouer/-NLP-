
# 数据集划分：训练/验证/测试
## 小数据量(<10000)
- 无验证集：70% 30%
- 有验证集：60% 20% 20%
## 大数据量
- 100万：98% 1% 1%
- 超百万：99.5% 0.25% 0.25%

# 方差偏差权衡
- 偏差反应学习算法对数据的拟合程度
- 方差反应算法在数据扰动下的表现
偏差和方差的权衡十分重要
- 存在高偏差：
    - 扩大网络规模，如添加隐藏层或隐藏单元数目
    - 寻找合适的网络架构，使用更大的 NN 结构
    - 花费更长时间训练
- 存在高方差;
    - 获取更多的数据
    - 正则化（regularization）
    - 寻找更合适的网络结构

# 正则化
## 逻辑回归中的正则化
正则化是在成本函数中加入一个正则化项，惩罚模型的复杂度。正则化可以用于解决高方差的问题
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/正则化.png)

其中，λ 为正则化因子，是超参数。
由于 L1 正则化最后得到 w 向量中将存在大量的 0，使模型变得稀疏化，因此 L2 正则化更加常用。

## 神经网络中的正则化
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/神经网络正则化1.png)
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/神经网络正则化2.png)

## 正则化的解释
加入正则化项，会降低W的取值，L1导致W很多分量为0，L2导致W很多分量都很小。W减小，则Z也相应减小，调节λ的大小，会将激活值调整到激活函数处于线性的部分，降低模型复杂度

## DropOut正则化
dropout（随机失活）是在神经网络的隐藏层为每个神经元结点设置一个随机消除的概率，保留下来的神经元形成一个结点较少、规模较小的网络用于训练。dropout 正则化较多地被使用在计算机视觉（Computer Vision）领域。

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/dropout.png)

在测试和预测阶段不适用dropout，其实就相当于在训练的时候每次训练一个随机失活后得到的小网络，在测试和预测阶段，对着些小网络做一个集成，集成方法明显可以降低方差。
对于单个神经元，其工作是接收输入并产生一些有意义的输出。但是加入了 dropout 后，输入的特征都存在被随机清除的可能，所以该神经元不会再特别依赖于任何一个输入特征，即不会给任何一个输入特征设置太大的权重。

## 早停止（Early Stopping）
将训练集和验证集进行梯度下降时的成本变化曲线画在同一个坐标轴内，在两者开始发生较大偏差时及时停止迭代，避免过拟合。这种方法的缺点是无法同时达成偏差和方差的最优。

# 标准化输入
使用标准化处理输入 X 能够有效加速收敛。

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/标准化.png)
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/标准化2.png)

# 梯度消失和爆炸
在梯度函数上出现的以指数级递增或者递减的情况分别称为梯度爆炸或者梯度消失。

假定 g(z)=z,b[l]=0，对于目标输出有：

y^=W[L]W[L−1]...W[2]W[1]X
- 对于 W[l]的值大于 1 的情况，激活函数的值将以指数级递增；
- 对于 W[l]的值小于 1 的情况，激活函数的值将以指数级递减。

对于导数同理。因此，在计算梯度时，根据不同情况梯度函数会以指数级递增或递减，导致训练导数难度上升，梯度下降算法的步长会变得非常小，需要训练的时间将会非常长。

权重的初始化：

- **初始化为0：** 会导致同层所有神经元的输出相同，那么在进行反向传播时，所有神经元的梯度相同，权重更新相同，结果就相当于同一个神经元被现行叠加了很多次。
- **随机初始化：** 很常用的方法，但是一旦选取的分布不当，会导致优化困难。假如每一层的权重都从标准正太分布中上一级取样，那么，随着层数的增加，每一层的输出值会迅速向0靠拢。而在反向传播时，根据链式法则，gradient等于当前函数的gradient乘以后一层的gradient，这意味着输出值x是计算gradient中的乘法因子，直接导致gradient很小，使得参数难以被更新。
 - **Xavier initialization:** 解决了上述随机初始化的问题。其基本思想是要保证当前层的输出与其输入的方差保持一致，这样就避免了所有输出值都趋向于0。具体推导在[这里](http://www.cnblogs.com/marsggbo/p/7462682.html)。
    - **tanh:**  Wl = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(1./layers_dims[l-1])
    - **relu:** Wl = np.random.randn(layers_dims[l],layers_dims[l-1])*np.sqrt(2./layers_dims[l-1])（He initialization）
- **Batch norm：** todo


