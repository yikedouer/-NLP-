# 梯度下降
https://github.com/yikedouer/-NLP-/blob/master
- batch 梯度下降法：
    - 对所有 m 个训练样本执行一次梯度下降，每一次迭代时间较长，训练过程慢
    - 相对噪声低一些，幅度也大一些
    - 成本函数总是向减小的方向下降
- 随机梯度下降法：
    - 对每一个训练样本执行一次梯度下降，训练速度快，但丢失了向量化带来的计算加速
    - 有很多噪声，减小学习率可以适当
    - 成本函数总体趋势向全局最小值靠近，但永远不会收敛，而是一直在最小值附近波动
- mini-batch梯度下降法:
    - 选择一个1 < size < m的合适的大小进行 Mini-batch梯度下降，可以实现快速学习，也应用了向量化带来的好处，且成本函数的下降处于前两者之间。
    - mini-batch 大小的选择:
        - 如果训练样本的大小比较小，如 m ⩽ 2000 时，选择 batch 梯度下降法；
        - 如果训练样本的大小比较大，选择 Mini-Batch 梯度下降法。为了和计算机的信息存储方式相适应，代码在 mini-batch 大小为 2 的幂次时运行要快一些。典型的大小为64,128,256,512
    
    mini-batch 的大小也是一个重要的超变量，需要根据经验快速尝试，找到能够最有效地减少成本函数的值。
    使用mini-batch前要先将数据集打乱，保证随机取样也符合整体样本的分布

**存在的问题：**
- 难以选择学习率，太小则网络收敛太慢，太大则损失函数会在最小点附近震荡，难以收敛
- 对特征向量中的所有的特征都采用了相同的学习率，如果训练数据十分稀疏并且不同特征的变化频率差别很大，这时候对变化频率慢得特征采用大的学习率而对变化频率快的特征采用小的学习率是更好的选择。
- 鞍点问题。在鞍点处各方向梯度均接近0，但既不是最小值也不是最大值

# 指数加权平均
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/指数加权平均1.png)
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/指数加权平均2.png)

**偏差修正：**

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/偏差修正.png)

**对所有之前的元素做加权平均，权重是指数级递减的，离当前位置越远的元素权重越低。当β=0.9时，当前元素之前的第10个元素的权重为0.9^(1/(1-0.9) = 0.9^10 ~ 1/e 已经衰减的很厉害了，再之前的就更不重要了。所以指数加权平均相当于用之前的1/(1-β)个数来算平均值。**

# Adagrad算法
每一步计算累计的梯度平方，用来改变学习率

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/adagrad.png)

Adagrad只是简单粗暴的对梯度进行平方累加，没有用到指数加权。如果目标函数有关自变量中某个元素的偏导数一直都较大，那么就让该元素的学习率下降快一点；反之，如果目标函数有关自变量中某个元素的偏导数一直都较小，那么就让该元素的学习率下降慢一点。然而，**由于s一直在累加按元素平方的梯度，自变量中每个元素的学习率在迭代过程中一直在降低（或不变）。** 所以，当学习率在迭代早期降得较快且当前解依然不佳时，**Adagrad在迭代后期由于学习率过小，可能较难找到一个有用的解。**

# 动量梯度下降（Momentum）
传统梯度下降针对某一点的损失函数再其中某一个方向弯曲特别厉害，其他方向较为平缓时存在问题。再这种点，弯曲厉害的方向的梯度会更大，梯度下降时会更侧重朝那个方向更新，这就导致学习率不能太高。

momentum法就是对梯度做指数加权平均。

    vdW[l]=βvdW[l]+(1−β)dW[l] 
    
    vdb[l]=βvdb[l]+(1−β)db[l]
    
    W[l]:=W[l]−αvdW[l]
    
    b[l]:=b[l]−αvdb[l]
    
这样，由于目标函数在弯曲厉害的方向上摇摆，所以在该方向上前后两次梯度方向相反，指数加权后，会正负抵消掉一部分，减缓震荡，在其他弯曲不厉害的方向上进行正叠加，加速收敛。



# RMSProp 算法
RMSProp（Root Mean Square Prop，均方根支）算法是在对梯度进行指数加权平均的基础上，引入平方和平方根。（或者说是对Adagrad进行了修改，不累加梯度的平方，而是指数加权梯度的平方）
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/RMSProp.png)
 
 由于变量s可看作是最近1/(1−β)个时刻的平方项g⊙g的加权平均，自变量每个元素的学习率在迭代过程中避免了“直降不升”的问题。
 
 # Adadelta
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/adadelta.png)
 
 # Adam
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/adam.png)
 
 
 **高级优化算法总体从两个方向来提高优化：**
 - 加入动量的思想，用指数加权平均后的梯度代替原有梯度，减少震荡。代表算法Momentum
 - 通过自动调整学习率来加快收敛，通过对每一步的梯度的平方进行叠加或者指数加权叠加，放在学习率的分母，动态调整学习率代表算法Adagrad，RMSprop
 - 
Adam算法对两种思想进行整合

# 学习率衰减

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/学习率衰减.png)

# 局部最优问题

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/局部最优.png)