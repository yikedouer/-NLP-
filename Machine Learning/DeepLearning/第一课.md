# 1. 激活函数
## 激活函数的使用是为了引入非线性

如果不加入线性激活函数的话，每一层输出都是上一层输入的线性组合，无论网络有多深，最后都是输入的线性组合
- **Sigmoid**
    - 优点：
        - 便于求导，函数平滑
        - 能压缩数据
        - 适用于前向传播
    - 缺点：
        - 容易出现梯度消失，当激活函数接近饱和区时，变化太缓慢，导数接近0，根据后向传递的数学依据是微积分求导的链式法则，当前导数需要之前各层导数的乘积，几个比较小的数相乘，导数结果很接近0，从而无法完成深层网络的训练。
        - 输出不是0均值（zero-centered）的：这会导致后层的神经元的输入是非0均值的信号，这会对梯度产生影响。以 f=sigmoid(wx+b)为例， 假设输入均为正数（或负数），那么对w的导数总是正数（或负数），这样在反向传播过程中要么都往正方向更新，要么都往负方向更新，导致有一种捆绑效果，使得收敛缓慢。
- **tanh**
    - tanh函数将输入值压缩到 -1~1 的范围，因此它是0均值的，解决了Sigmoid函数的非zero-centered问题，但是它也存在梯度消失和幂运算的问题。
- **ReLU**
    - 优点：
        - 收敛速度快，梯度不会饱和，解决了梯度消失问题
        - 计算复杂度低，不需要进行指数运算
        - 适合用于后向传播
    - 缺点：
        - ReLU的输出不是zero-centered
        - Dead  ReLU  Problem（神经元坏死现象）：某些神经元可能永远不会被激活，导致相应参数永远不会被更新（在负数部分，梯度为0）。产生这种现象的两个原因：参数初始化问题；learning  rate太高导致在训练过程中参数更新太大。 解决方法：采用Xavier初始化方法，以及避免将learning  rate设置太大或使用adagrad等自动调节learning  rate的算法。
        - ReLU不会对数据做幅度压缩，所以数据的幅度会随着模型层数的增加不断扩张。
- **Leaky ReLU**
    - a = max(0.01z,z)
    - 优点：
        保证了z<0时，梯度不为0
        理论上要比ReLU好，但在实际使用时病没有好于ReLU，因此不太常用
- **激活函数的导数**
    ![](D:/Program/YNote/workspace/gorpel@163.com/Picture/导数.png)

# 2. 梯度下降
## 前向传播：
![image](https://github.com/yikedouer/-NLP-/tree/master/Picture/前向传播.png)
## 反向传播
![image](https://github.com/yikedouer/-NLP-/tree/master/Picture/反向传播.png)
# 3. 随机初始化
    
如果将参数全部初始化为零，那么同一层的神经元计算结果完全相同，同时，各神经元对输出节点的影响也相同，这样会导致反向传播时的导数相同，所有节点都是对称的，相当于计算的是同一个函数
