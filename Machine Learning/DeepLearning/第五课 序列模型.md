# 循环神经网络(RNN)

## 网络模型
对于序列数据，使用标准神经网络存在以下问题：
- 对于不同的示例，输入和输出可能有不同的长度，因此输入层和输出层的神经元数量无法固定。
- 从输入文本的不同位置学到的同一特征无法共享。
- 模型中的参数太多，计算量太大。

于是引入RNN：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn2.png)

## 反向传播

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn反向传播.png)

## 梯度消失（爆炸）
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn梯度消失.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn梯度消失2.png)
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/rnn梯度消失3.png)

## LSTM 和 GRU
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/LSTM.jpg)

## 双向循环神经网络（BRNN）

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/双向rnn.png)

## 深度循环神经网络（DRNN)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/深度rnn.png)

对前向和后向得到的隐层状态向量进行拼接操作

# 自然语言处理与词嵌入

## 神经概率语言模型

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/神经概率语言模型.png)

输入n个词的one-hot表示，分别与投射矩阵C做矩阵乘法，得到n个稠密表示，将这n个稠密向量进行拼接，输入隐藏层，输出层经softmax得到输出

这里的C矩阵就是我们想要得到的词向量矩阵

## word2vec
word2vec是简单高效的词嵌入学习算法，分为skip-gram 和 CBOW 两种模型，以及负采样和层次softmax两种近似训练方法

word2vec在NNLM的基础上进行了改进：
- 使用滑动窗口
- 在输入的n个one-hot表示的词经过投射矩阵C后得到的n个稠密向量不再拼接，而是相加取平均，这相当于忽略了窗口内词的顺序，同等看待
- 去除隐层，直接传入softmax

### skip-gram

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/skip-gram.png)

### CBOW

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/cbow.png)

### 负采样

[参考](http://www.hankcs.com/nlp/word2vec.html)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/负采样1.png)、

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/负采样2.png)

### 层次softmax

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/层次softmax.png)


### 词对和短语

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/词组.png)

根据上面的公式为词组计算得分，设定阈值，高于阈值的就将之作为词组处理


### 高频词二次采样

对高频词根据其在训练数据中出现的频率进行降采样

## GloVe 和 FastText

### GloVe

word2vec并没有考虑到全局信息。
GloVe首先会生成一个全局共现矩阵，考虑三个词i，j，k。通过计算f(i,k)和f(j,k)的比值，就可以发现k是更接近i还是j。

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/glove.png)

损失函数则是：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/glove2.png)

### FastText

word2vec中，是以词为单位进行学习，而通常一个词的有很多不同的形态，word2vec把他们都当成独立的词来看待。
FastText在使用负采样的skip-gram的基础上，将每个中心词用它的所有n-gram子词的bag of word来表示，对每一个n-gram学习一个向量，而词向量用这些n-gram的向量求和得到。很重要的一点是，不同词之间，共享n-gram向量，但通过在每个词前后加入 < 和 > 来对子词和独立词进行区分。

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/fasttext.png)

这样做可以有效适应词的形态变化，而且，在处理低频词时十分有利，对词典外的新词也可以获得不错的词向量。


# Seq2Seq

## 简单Seq2Seq

Seq2Seq（Sequence-to-Sequence）模型能够应用于机器翻译、语音识别等各种序列到序列的转换问题。一个 Seq2Seq 模型包含编码器（Encoder）和解码器（Decoder）两部分。

Seq2Seq中，Encoder的RNN将输入序列编码成一个固定维度的向量C(也就是最后一个RNN单元的输出)，将C传入Decoder的RNN，按照C的使用方式，一般分为两种：
- Decoder RNN中，每个时间t的隐层变量的输入分别分上一时刻的输出值，上一时刻的隐层变量值和C，该时刻的输出则由隐层状态，上一时刻输出和C为输入的softmax得到

    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/seq1.png)
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/seq.jpg)
    
- Decoder RNN中，C只作为初始隐层状态，后续不再参与计算，每一层的隐层状态以上一时刻的隐层状态和上一时刻输出作为输入

    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/seq3.png)
**在两组模型中都在每个句子末尾添加了特殊的结束符号EOS，在Decoder中，预测出EOS则结束**

## 带Attention的Seq2Seq

Attention的思想在于，在解码过程中，每一个时间步都用到了相同的由全部输入序列得到的编码向量C，但更合理的情况是，在解码时刻t，可能会更多的用到了某一段输入序列，而其他序列信息用的较少，也就是在不同时间步，对输入序列分配了不同的注意力。

Encoder部分不变(在原论文中其实是有变化，采用了双向RNN)，调整主要在Decoder：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/attention.png)

其中的a函数有多重设计方法，在原始论文中，**eij = V * tanh(Ws * si-1 + Wh * hj)**

## Beam Search
在Seq2Seq中，经常使用Beam Search来得到最优解

在Seq2Seq中，我们的最终目的是找到最可能的输出序列y1，y2，...,yt,也就是找到最大的P(y1,y2,...,yt)。假设词表长度为|V|，输出序列长度为t,则我们的任务是在|V|^t中选出最优解，这样复杂度太高。我们之前的做法是贪心搜索(Greedy Search),一步一步选择，每一步选择当前步中计算出来的概率最大的y，但明显这很有可能错过最优解，这样的复杂度为|V|*t。

Beam Search介于两者之间，它的思想是每一步记录下多个可能的结果，假设束宽为3，则每一步记录3个最可能的。
- 第一步，记录3个可能的值yi,yj,yk
- 第二步，遍历词表，分别计算给定前一个输出为yi的V中结果，给定前一个输出为yj的V中结果和给定前一个输出为yk的V中结果，在这3V中结果中选择最大的三个。
- 以后的每一步都这样计算。
- 最后，把每一步中得到的3个序列汇总，从中选出以特殊结束符EOS结尾的序列作为候选，再从候选序列中取以下公式计算得分最高的序列：

    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/beam.png)
    
其中取log是为了防止连乘造成下溢出，L为候选序列长度，α一般可选为0.75。分母上的Lα是为了惩罚较长序列的分数中的对数相加项，因为如果不加限制的话，会趋向于选择长度更长的，长度越长log相加的结果越大。