# 超参数调试
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/超参数.png)

# Batch Normalization

**会使参数搜索问题变得很容易，使神经网络对超参数的选择更加稳定，超参数的范围会更庞大，工作效果也很好，也会使训练更容易。通常在激活函数前进行。**

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/批归一化1.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/批归一化2.png)

**设置 γ 和 β 的原因是，如果各隐藏层的输入均值在靠近 0 的区域，即处于激活函数的线性区域，不利于训练非线性神经网络，从而得到效果较差的模型。因此，需要用 γ 和 β 对标准化后的结果做进一步处理。**

## BN的起因及作用的：

### 起因：
- 当底层网络中参数发生微弱变化时，由于每一层中的线性变换与非线性激活映射，这些微弱变化随着网络层数的加深而被放大，非常容易引起梯度饱和
- 参数的变化导致每一层的输入分布会发生改变，进而上层的网络需要不停地去适应这些分布变化，使得我们的模型训练变得困难。上述这一现象叫做Internal Covariate Shift。

### 作用：
- 通过对隐藏层各神经元的输入做类似的标准化处理，提高神经网络训练速度；
- 可以使前面层的权重变化对后面层造成的影响减小，整体网络更加健壮。
    - 如果不同的mini-batch的数据分布不同，那么参数就要不断调整进行适应，降低学习效率
    - 如果训练数据和实际数据分布不同，那么没有BN的话就要重新训练

### 测试阶段使用
在测试阶段，我们可能只需预测一个样本，那么对着一个样本的均值和方差的估计就是有偏估计。

我们可以将每个mini-batch中得到的均值和方差，利用指数加权平均得到全局数据的均值和方差（也可以保存每组mini-batch的均值方差进行无偏估计）

### Batch Normalization的优势：
- **BN使得网络中每层输入数据的分布相对稳定，加速模型学习速度**
    
    BN通过规范化与线性变换使得每一层网络的输入数据的均值与方差都在一定范围内，使得后一层网络不必不断去适应底层网络中输入的变化，从而实现了网络中层与层之间的解耦，允许每一层进行独立学习，有利于提高整个神经网络的学习速度。
- **BN使得模型对网络中的参数不那么敏感，简化调参过程，使得网络学习更加稳定**
- **BN允许网络使用饱和性激活函数（例如sigmoid，tanh等），缓解梯度消失问题**

    在不使用BN层的时候，由于网络的深度与复杂性，很容易使得底层网络变化累积到上层网络中，导致模型的训练很容易进入到激活函数的梯度饱和区；通过normalize操作可以让激活函数的输入数据落在梯度非饱和区，缓解梯度消失的问题；另外通过自适应学习  与  又让数据保留更多的原始信息。
- **BN具有一定的正则化效果**
    
    每组mini-batch得到的均值方差来自局部数据，对整体数据而言有一定的噪声，随机噪声有正则化的效果