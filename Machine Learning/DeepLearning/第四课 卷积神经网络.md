[参考笔记](http://kyonhuang.top/Andrew-Ng-Deep-Learning-notes/#/Convolutional_Neural_Networks/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C?id=%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97)
# 卷积
## 卷积运算
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/卷积运算.png)
卷积核(滤波器)相当于一个特征模板，对每一个区域做卷积运算相当于看该区域的像素点们有多满足这种特征

## 填充(Padding)
假设输入图片的大小为 **n×n**，而滤波器的大小为 **f×f**，则卷积后的输出图片大小为 **(n−f+1)×(n−f+1)**，由此带来两个问题：
- 每次卷积运算后，输出图片的尺寸缩小
- 原始图片的角落、边缘区像素点在输出中采用较少，输出图片丢失边缘位置的很多信息
因此可以在进行卷积操作前，对原始图片在边界上进行填充（Padding），以增加矩阵的大小。通常将 0 作为填充值。
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/填充.png)

设每个方向扩展像素点数量为 **p**，则填充后原始图片的大小为 **(n+2p)×(n+2p)**，滤波器大小保持 **f×f**不变，则输出图片大小为 **(n+2p−f+1)×(n+2p−f+1)**。
在进行卷积运算时，我们有两种选择：
- Valid 卷积：不填充，直接卷积。结果大小为 **(n−f+1)×(n−f+1)**
- Same 卷积：进行填充，并使得卷积后结果大小与输入一致，这样 **p=(f−1)/2**

## 卷积步长
卷积过程中，有时需要通过填充来避免信息损失，有时也需要通过设置步长（Stride）来压缩一部分信息。

步长表示滤波器在原始图片的水平方向和垂直方向上每次移动的距离。之前，步长被默认为 1。而如果我们设置步长为 2，则卷积过程如下图所示：
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/卷积步长1.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/卷积步长2.png)

## 高维卷积
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/高维卷积.png)

## 单层卷积及符号总结
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/单层卷积.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/符号总结.png)

# 池化
池化层的作用是缩减模型的大小，提高计算速度，同时减小噪声提高所提取特征的稳健性。

采用较多的一种池化过程叫做最大池化（Max Pooling）。将输入拆分成不同的区域，输出的每个元素都是对应区域中元素的最大值，如下图所示：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/池化.png)

池化过程类似于卷积过程，上图所示的池化过程中相当于使用了一个大小 f=2的滤波器，且池化步长 s=2。卷积过程中的几个计算大小的公式也都适用于池化过程。如果有多个通道，那么就对每个通道分别执行计算过程。

对最大池化的一种直观解释是，元素值较大可能意味着池化过程之前的卷积过程提取到了某些特定的特征，池化过程中的最大化操作使得只要在一个区域内提取到某个特征，它都会保留在最大池化的输出中。但是，没有足够的证据证明这种直观解释的正确性，而最大池化被使用的主要原因是它在很多实验中的效果都很好。

另一种池化过程是平均池化（Average Pooling），就是从取某个区域的最大值改为求这个区域的平均值：
![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/池化2.png)

池化过程的特点之一是，它有一组超参数，但是并没有参数需要学习。池化过程的超参数包括滤波器的大小 f、步长 s，以及选用最大池化还是平均池化。而填充 p则很少用到。

池化过程的输入维度为：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维度1.png)

输出维度为

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维度.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/实例.png)

## 使用卷积的原因

相比标准神经网络，对于大量的输入数据，卷积过程有效地减少了 CNN 的参数数量，原因有以下两点：
- **参数共享（Parameter sharing）**：特征检测如果适用于图片的某个区域，那么它也可能适用于图片的其他区域。即在卷积过程中，不管输入有多大，一个特征探测器（卷积核或滤波器）就能对整个输入的某一特征进行探测。
- **稀疏连接（Sparsity of connections）**：在每一层中，由于滤波器的尺寸限制，输入和输出之间的连接是稀疏的，每个输出值只取决于输入在局部的一小部分值。

**池化过程则在卷积后很好地聚合了特征，通过降维来减少运算量。**

由于 CNN 参数数量较小，所需的训练样本就相对较少，因此在一定程度上不容易发生过拟合现象。并且 CNN 比较擅长捕捉区域位置偏移。即进行物体检测时，不太受物体在图片中位置的影响，增加检测的准确性和系统的健壮性。

# 经典深度卷积网络
 ## LeNet
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/lenet.png)
 
 ## AlexNet
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/alexnet.png)
 
 ## VGG-16
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/vgg.png)
 
 ## 残差网络
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/残差.png)
 
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/残差2.png)
 
 ## 1 X 1卷积
 ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/1x1.png)
 
 ## Inception
 ### TODO