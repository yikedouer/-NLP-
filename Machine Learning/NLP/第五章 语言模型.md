
# 语言模型
用来计算一个句子的概率

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/语言模型.png)

随着历史词的增加，可能性指数级增长，第一个词有|V|种可能，计算第二个词时还要考虑之前的词，可能性变为|V| * |V|，以此类推

## n-gram模型

n=gram指，在计算第n个词时，只考虑之前的n-1个词，也叫n-1阶马尔科夫链

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/bigram.png)

每一个条件概率可以通过最大似然法来估计：

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/最大似然.png)

## 语言模型评价指标

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/评价.png)

## 数据平滑

在计算n-gram时，会出现很多0
概率，因为有些n-gram没有出现杂我们的语料中，但模型不应该简单的将之算为0，由此引入了数据平滑。

[这篇](http://www.shuang0420.com/2017/03/24/NLP%20%E7%AC%94%E8%AE%B0%20-%20%E5%B9%B3%E6%BB%91%E6%96%B9%E6%B3%95(Smoothing)%E5%B0%8F%E7%BB%93/)博客讲的很好。

平滑方法一般分为两种思想，插值和回退。
- 插值，就是把不同阶的模型结合起来
- 回退，直观的理解，就是说如果没有 3gram，就用 bigram，如果没有 bigram，就用 unigram
两者区别和联系：
- 区别：在计算不为0的概率时，差值模型使用低阶模型的信息，而回退模型则不使用
- 联系：对0概率进行平滑时，两者都用了低阶模型的信息

与差值平滑算法相比，回退算法所需参数较少，而且可以直接确定，无需通过某种迭代重估算法反复训练，因此实现更加方便。

### 加法平滑
#### 加1平滑(拉普拉斯平滑)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/加1.png)

#### 加δ平滑(Additive smoothing)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/加.png)

### 古德-图灵估计

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/gude.png)

### Katz平滑

是典型的回退方法

古德-图灵法时通过缩减非零概率的值，然后把所有缩减得到的值平均分配给0概率，Katz是在缩减的基础上，不平均分配，而是按照低阶的分布进行分配

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/katz.png)

#### Jelinek-Mercer平滑

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/je1.png)

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/je2.png)

#### 绝对减值法

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/绝对减值.png)

### 平滑方法的比较
- 最好的是 Kneser-Ney
- 第二位是Katz和J-M
    - 数据量大的话Katz效果好
    - 数据量小J-M好一点