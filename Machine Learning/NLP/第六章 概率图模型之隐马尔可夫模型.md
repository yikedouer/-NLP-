# 生成式模型与判别式模型

**生成式模型：**
- 假设y决定x：状态序列y以一定的概率生成观测序列x，针对联合概率分布p(x,y)建模，并通过估计使生成概率最大的生成序列来获取y
- 严格的独立性假设，特征是事先给定的，并且特征之间的关系直接体现在公式中
- 处理单类问题比较灵活，模型变量之间的关系比较清楚，模型可以通过增量学习获得，课用于数据不完整的情况。缺点在于模型鼻尖哦复杂，推到和学习都比较困难
- n-gram语言模型，HMM，朴素贝叶斯等

**判别式模型：**
- 假设x决定y，直接对后延概率p(y|x)建模，从x中提取特征，学习模型参数，是的条件概率符合一定形式的最优
- 特征可以任意给定，一般通过函数表示
- 优点是处理多类问题或分辨一类与其他类之间的差异时比较灵活，模型简单，容易建立和学习，弱点在于模型的描述能力有限，变量之间的关系不清楚，且大多数判别式方法为有监督学习，不能扩展成无监督学习
- LR，SVM，最大熵模型，CRF

# 隐马尔科夫模型(HMM)

## 五元素：
- 隐状态数目**N**
- 从每个状态可能输出的符号数目**M**
- 状态转移矩阵**A**
- 状态到符号的概率分布矩阵(符号发射矩阵)**B**
- 初始状态概率分布**π**

## 两个假设
- **齐次马尔可夫假设：** 当前隐含状态只与前一时刻隐藏状态有关
- **观测独立性假设：** 任一时刻观测只依赖于当前时刻的隐含状态

![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/hmm.png)

## 三个问题
- **估计问题：** 给定观测序列**O**和模型**μ=(A,B,π)**，如何快速计算给定模型**μ**下，观测序列O的概率**P(O|μ)**  ————**前向后向算法**
- **预测问题：** 给定观测序列**O**和模型**μ=(A,B,π)**，如何快速有效地选择在一定意义下的最优状态序列**Q** ————**维特比算法**
- **参数估计问题：** 给定观测序列**O**，如何根据最大似然估计求得模型的参数，即如何调节**μ=(A,B,π)**，使得**P(O|μ)** 最大 ———**Baum-Welch算法**

    ### 估计问题
    **直接计算法：** 
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/直接计算1.png)
    
    **前向后向算法：** 本质上是动态规划。
    - 定义一个前向变量 **α** (每一时刻都是一个 **N** 维向量，对应 **N** 个状态)，用来记录到时刻 **t** ，观测序列是 **(O1,O2,...,Ot)** ,且当前状态是 **qt=si** 的概率，初始化为 **α1(i) = π(i) * bi(O1)**
    - 递推公式:在下一时刻，要分别计算α的N个分量，每个分量按如下公式计算：
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/递推.png)
    
    第 **i** 个分量由前一时刻的 **N** 个 **α** 分量分别乘以对应的转移概率，然后求和，再乘以该状态发射出当前观测符号的概率
    
    - 终止状态对终止时刻 **α** 的每一个分量求和，得到最终概率
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/求和.png)
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/格子.png)
    
    后向算法与前向算法原理相同，不过是从后向前算：
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/后向1.png)
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/后向2.png)
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/概率.jpg)
    
    ### 预测问题(解码问题)
    
    本质也是动态规划算法
    - 定义一个维特比变量 **δt** (**N** 维向量，对应 **N** 个状态)代表之前的观测序列是**(O1,O2,...,Ot)**,隐状态是**(q1,q2,...,qt-1)**,且当前隐状态 **qt = si** 的概率
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维特比1.png)
    
    定义一个路径变量 **Ψt**(**N** 维) 用于记录路径
    
    - 递推公式:
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维特比2.png)
    
    时间复杂度与前向后向算法一样，是**O(N^2*T)**
    在实际应用中通常不只搜索一个最优状态序列，而是搜索**n**个最佳，因此在每个节点要记录**m** (**m<n**)个最佳状态
    
    - 终结：最终时刻T得到**δt**变量，在他的**N**个分量中选择最大的
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维特比3.png)
    
    - 路径回溯
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/维特比4.png)
    
    ### 参数估计问题
    
    给定观测序列，估计**μ=(A,B,π)**，使得**P(O|μ)** 最大：
        **argmax(P(O<font size=2>training</font>|μ))**
        
    #### 监督学习方法
    通常是根据标记好的训练数据进行极大似然估计
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/EM1.png)
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/EM2.png)
    
    由于监督学习需要标注好的训练数据，人工标注代价很大，因此很多时候会采用无监督学习算法。
    
    #### 无监督—— EM算法
    
    观测序列 **O** 为可观测序列，状态序列I为不可观测的隐含序列，因此，**HMM** 的参数估计问题其实是含有因变量的估计问题
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/EM4.png)
    
    **HMM** 的参数可以有 **EM** 算法估计出来
    
    ###### EM算法
    
    **EM的详细推导见MachineLearning文件夹下的EM专题**
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/BW1.png)
    
    
    
    ![image](D:/Program/YNote/workspace/gorpel@163.com/Picture/BW2.png)
    

**HMM缺点：**
- 需要对联合概率建模，计算繁琐，模型复杂
- 对观测序列做了独立性假设，这个假设太强，没有考虑到观测序列之间的关系
- 难以考虑除了顺序意外的其他特征，比如字母是否大写，是否包含阿拉伯数字等
- 标记偏置问题
  